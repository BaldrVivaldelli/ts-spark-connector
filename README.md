# ts-spark-connector
ğŸŒ± **Status: Alpha â€“ Early growth stage**

TypeScript client for [Apache Spark Connect](https://spark.apache.org/docs/latest/sql-connect.html).  
Construct Spark logical plans entirely in TypeScript and run them against a Spark Connect server.

## ğŸš€ Features

- Build Spark logical plans using a fluent, PySpark-style API in TypeScript
- Evaluate transformations locally or stream results via Arrow
- Tagless Final DSL design with support for multiple backends
- Composable, immutable, and strongly typed DataFrame operations
- Supports column expressions (`col`, `.gt`, `.alias`, `.and`, etc.)
- Compatible with Spark Connect Protobuf and `spark-submit --class org.apache.spark.sql.connect.service.SparkConnectServer`
- **Supports Spark SetOperations** (`UNION`, `INTERSECT`, `EXCEPT`) with `by_name`, `is_all`, and `allow_missing_columns`
- Support for Spark-compatible joins with configurable join types
- Session-aware execution without relying on global singletons
- Includes **ready-to-run examples** in the `examples/` folder

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-org/ts-spark-connector
cd ts-spark-connector
npm install
```

## ğŸ”§ Docker Setup

You need a Spark Connect server running. Example `docker-compose.yml`:

```yaml
services:
  spark:
    build: ./spark-server
    ports:
      - "15002:15002"
    volumes:
      - ./example_data:/data
    environment:
      - SPARK_NO_DAEMONIZE=true
```

`spark-server/Dockerfile`:
```Dockerfile
FROM bitnami/spark:latest
USER root
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh
CMD ["/entrypoint.sh"]
```

`spark-server/entrypoint.sh`:
```bash
#!/bin/bash
/opt/bitnami/spark/bin/spark-submit   --class org.apache.spark.sql.connect.service.SparkConnectServer   --conf spark.sql.connect.enable=true   --conf spark.sql.connect.grpc.binding=0.0.0.0:15002   /opt/bitnami/spark/jars/spark-connect_2.12-4.0.0.jar
```

Example data in `/example_data`:
- `people.tsv`
- `purchases.tsv`

## ğŸ“‚ Examples

We include a set of TypeScript scripts in the `examples/` folder:

| File              | Description |
|-------------------|-------------|
| `join.ts`         | Inner join between two datasets |
| `groupBy.ts`      | Aggregations with `groupBy` and `orderBy` |
| `withColumn.ts`   | Adding and renaming columns |
| `union.ts`        | `SetOperation` with UNION queries |
| `distinct.ts`     | Using `distinct()` and `dropDuplicates()` |

Run any example with:

```bash
npx tsx examples/join.ts
```

## ğŸ§ª Quick Usage

```ts
import { spark } from "./src/spark/session";
import { col } from "./src/engine/column";

const people = spark.read
    .option("delimiter", "\t")
    .option("header", "true")
    .csv("/data/people.tsv");

const purchases = spark.read
    .option("delimiter", "\t")
    .option("header", "true")
    .csv("/data/purchases.tsv");

people
  .join(purchases, col("id").eq(col("user_id")), "left")
  .select("name", "product", "amount")
  .filter(col("amount").gt(100))
  .show();
```

### Example: UNION with SetOperation

```ts
const p2024 = purchases.filter(col("year").eq(2024));
const p2025 = purchases.filter(col("year").eq(2025));

p2024.union(p2025, { is_all: true, by_name: false })
  .limit(5)
  .show();
```

## ğŸ’¡ Column Expressions

```ts
col("age").gt(18)
col("country").eq("AR")
col("age").gt(18).and(col("active").eq(true)).alias("eligible")
```

## ğŸ§  Tagless Final DSL

```ts
function userQuery<F>(dsl: DataFrameDSL<F>): F {
  return dsl
    .select(["name", "age"])
    .filter("age > 18")
    .withColumn("eligible", col("age").gt(18).and(col("country").eq("AR")));
}
```

## âœ… Status Features & Roadmap

## Legend
- **P0** = Paridad base inmediata
- **P1** = I/O realista
- **P2** = Performance & DX
- **P3** = SQL/CatÃ¡logo & control
- **P4** = UDF (scalar / vectorizadas)
- **P5** = Streaming / Lakehouse / JDBC
- **P6** = MLlib
- **â€”** = ya implementado

## Feature Matrix

| Feature | Supported | Priority |
|---|---|---|
| CSV Reading | âœ… | â€” |
| Filtering | âœ… | â€” |
| Projection / Alias | âœ… | â€” |
| Arrow decoding | âœ… (`.show()` prints tabular output) | â€” |
| Column expressions | âœ… (`col`, `.gt`, `.and`, `.alias`, etc.) | â€” |
| DSL abstraction | âœ… Tagless Final | â€” |
| Join | âœ… Supports all join types | â€” |
| Aggregation | âœ… (`groupBy().agg({...})`) | â€” |
| Distinct | âœ… (`distinct()`, `dropDuplicates(...)`) | â€” |
| Sorting | âœ… (`orderBy(...)`, `sort(...)`) | â€” |
| Limit & Take | âœ… (`limit(n)`) | â€” |
| **SetOperation** | âœ… (`UNION`, `INTERSECT`, `EXCEPT`) | â€” |
| Column renaming | âœ… (`withColumnRenamed(...)`) | â€” |
| Type declarations | âœ… `.d.ts` published to NPM | â€” |
| Modular compiler core | âœ… (`engine/` separated from Spark backend) | â€” |
| NPM Package | âœ… [Published](https://www.npmjs.com/package/ts-spark-connector) | â€” |
| Tests (Unit + Integration) | ğŸš§ In progress | **P0** |
| **withColumn(...)** | âŒ Not yet | **P0** |
| **when(...).otherwise(...)** (CASE WHEN) | âŒ Not yet | **P0** |
| **Window functions** (`over`, `partitionBy`, `orderBy`, `rowsBetween`) | âŒ Not yet | **P0** |
| **Null handling** (`na.drop`, `na.fill`, `na.replace`, `isNull`) | âŒ Not yet | **P0** |
| **Parquet Reading** | âŒ Not yet | **P1** |
| **JSON Reading** | âŒ Not yet | **P1** |
| **DataFrameWriter** (CSV/JSON/Parquet/ORC) | âŒ Not yet | **P1** |
| Write `partitionBy`, `bucketBy`, `sortBy` | âŒ Not yet | **P1** |
| **describe()**, `summary()` | âŒ Not yet | **P2** |
| **unionByName(...)** | âŒ Not yet | **P2** |
| **Complex types** (arrays/maps/struct) + `explode/posexplode` | âŒ Not yet | **P2** |
| **JSON helpers** (`from_json`, `to_json`) | âŒ Not yet | **P2** |
| **cache() / persist() / unpersist()** | âŒ Not yet | **P2** |
| **repartition(...) / coalesce(...)** | âŒ Not yet | **P2** |
| **explain(...)** (`simple/extended/formatted`) | âŒ Not yet | **P2** |
| `SparkSession.builder.config(...)` | âŒ Not yet | **P2** |
| Auth/TLS for Spark Connect | âŒ Not yet | **P2** |
| **spark.sql(...)** | âŒ Not yet | **P3** |
| Temp views (`createOrReplaceTempView`) | âŒ Not yet | **P3** |
| Catalog (`read.table`, `saveAsTable`) | âŒ Not yet | **P3** |
| Plan viz / AST dump | âŒ Not yet | **P3** |
| **Join hints** (`broadcast`, `shuffle_replicate_nl`, etc.) | âŒ Not yet | **P3** |
| **sample(...)**, `randomSplit(...)` | âŒ Not yet | **P3** |
| UDF (scalar) | âŒ Not yet | **P4** |
| **UDAF / Vectorized UDF (Arrow)** | âŒ Not yet | **P4** |
| Structured Streaming (`readStream` / `writeStream`) | âŒ Not yet | **P5** |
| Watermark / trigger / output modes | âŒ Not yet | **P5** |
| Lakehouse: Delta/Iceberg/Hudi (`format(...)`) | âŒ Not yet | **P5** |
| JDBC read/write (`format("jdbc")`) | âŒ Not yet | **P5** |
| **MLlib** (Pipelines/Transformers/Estimators bÃ¡sicos) | âŒ Not yet | **P6** |

---

## Roadmap por etapas (P0 â†’ P6)

### P0 â€” Paridad base inmediata
**Objetivo:** igualar ergonomÃ­a mÃ­nima de PySpark para DataFrame/Column.  
**Incluye:** `withColumn`, `when/otherwise`, **window functions**, `na.*`, **tests** (unit + integraciÃ³n).  
**Criterios de aceptaciÃ³n:**
- âœ… `withColumn` soporta nuevas columnas y reemplazo.
- âœ… `when(...).otherwise(...)` compone expresiones condicionales en `select`/`withColumn`.
- âœ… Ventanas: `over(partitionBy(...).orderBy(...).rowsBetween(...))` con agregaciones.
- âœ… `na.drop/fill/replace` + `isNull/isNotNull` operativos.
- âœ… Suite de tests corriendo en CI contra Spark Connect local.
  **Notas:** Agregar ejemplos en README + `df.explain()` stub para depuraciÃ³n temprana si ayuda.

### P1 â€” I/O realista
**Objetivo:** trabajar con formatos comunes de datos y escribir resultados.  
**Incluye:** **Parquet/JSON reading**, **DataFrameWriter** (CSV/JSON/Parquet/ORC), `partitionBy/bucketBy/sortBy` (write).  
**Criterios:**
- âœ… `spark.read.parquet/json/csv(...)` con `options(...)` comunes.
- âœ… `df.write.format(...).mode("overwrite|append").save(...)`.
- âœ… `partitionBy` en escritura y smoke tests leyendo lo escrito.
  **Notas:** Cubrir inferencia de esquema bÃ¡sica y errores claros.

### P2 â€” Performance & DX
**Objetivo:** control de particiones, caching y mejor depuraciÃ³n.  
**Incluye:** `cache/persist/unpersist`, `repartition/coalesce`, **explain(formatted)**, `describe/summary`, `unionByName`, **complex types + explode**, **JSON helpers**, `SparkSession.builder.config`, **Auth/TLS**.  
**Criterios:**
- âœ… Cambios de particiÃ³n reflejados en el plan.
- âœ… `cache/persist` no rompe consistencia y `unpersist` limpia.
- âœ… `explain("formatted")`/`df.explain()` muestra plan vÃ¡lido.
- âœ… `from_json/to_json` + `explode` en arrays/maps/struct.
  **Notas:** Documentar recomendaciones de tuning y ejemplos de DX.

### P3 â€” SQL/CatÃ¡logo & control fino
**Objetivo:** interop total con SQL y catÃ¡logo.  
**Incluye:** **spark.sql(...)**, **temp views**, **catalog** (`read.table`, `saveAsTable`), **plan viz/AST dump**, **join hints**, `sample/randomSplit`.  
**Criterios:**
- âœ… `spark.sql("...")` produce `DataFrame` equivalente al DSL.
- âœ… `createOrReplaceTempView` usable desde `spark.sql`.
- âœ… `broadcast(df2)`/hints reflejados en el plan.
  **Notas:** Opcional: comando para volcar plan lÃ³gico en JSON/Protobuf.

### P4 â€” UDF (funciones definidas por usuario)
**Objetivo:** extender transformaciones con lÃ³gica propia.  
**Incluye:** **Scalar UDF**, **UDAF / vectorized UDF (Arrow)**.  
**Criterios:**
- âœ… Registro y uso de UDF en DSL y en `spark.sql`.
- âœ… Vectorized UDF con Arrow en path feliz y tests de interoperabilidad.
  **Notas:** Documentar costos/limitaciones y mejores prÃ¡cticas.

### P5 â€” Streaming / Lakehouse / JDBC
**Objetivo:** habilitar pipelines de producciÃ³n.  
**Incluye:** **Structured Streaming** (`readStream`/`writeStream`, **watermark**, **trigger**, **output modes**), **Delta/Iceberg/Hudi** vÃ­a `format(...)`, **JDBC** read/write.  
**Criterios:**
- âœ… Ejemplo endâ€‘toâ€‘end de streaming (socket/Kafka â†’ transform â†’ sink).
- âœ… Lectura/escritura a Delta/Iceberg/Hudi cuando el cluster tiene los jars.
- âœ… JDBC probado contra una base popular (Postgres/MySQL).

### P6 â€” MLlib
**Objetivo:** pipelines de ML sobre DataFrames desde TypeScript.  
**Incluye:** Pipelines, Estimators/Transformers bÃ¡sicos (e.g., `StringIndexer`, `VectorAssembler`, `LogisticRegression`).  
**Criterios:**
- âœ… Entrenar, guardar y cargar modelos; `transform()` sobre DataFrame.
- âœ… Ejemplo reproducible (dataset pÃºblico) y guÃ­a de migraciÃ³n desde PySpark.

---


## ğŸ“„ License

Apache-2.0 â€” This project aims to democratize access to Spark features from the TypeScript ecosystem.
