# ts-spark-connector

üå± **Status: Alpha ‚Äì Early growth stage**

TypeScript client for [Apache Spark Connect](https://spark.apache.org/docs/latest/sql-connect.html).  
Construct Spark logical plans entirely in TypeScript and run them against a Spark Connect server.

## üöÄ Features

- Build Spark logical plans using a fluent, PySpark-style API in TypeScript
- Evaluate transformations locally or stream results via Arrow
- Tagless Final DSL design with support for multiple backends
- Composable, immutable, and strongly typed DataFrame operations
- Supports column expressions (`col`, `.gt`, `.alias`, `.and`, etc.)
- Compatible with Spark Connect Protobuf and
  `spark-submit --class org.apache.spark.sql.connect.service.SparkConnectServer`
- **Supports Spark SetOperations** (`UNION`, `INTERSECT`, `EXCEPT`) with `by_name`, `is_all`, and
  `allow_missing_columns`
- Support for Spark-compatible joins with configurable join types
- Session-aware execution without relying on global singletons
- Includes **ready-to-run examples** in the `examples/` folder

## üì¶ Installation

```bash
git clone https://github.com/your-org/ts-spark-connector
cd ts-read-connector
npm install
```

## üîß Docker Setup

You need a Spark Connect server running. Example `docker-compose.yml`:

```yaml
services:
  spark:
    build: ./read-server
    ports:
      - "15002:15002"
    volumes:
      - ./example_data:/data
    environment:
      - SPARK_NO_DAEMONIZE=true
```

`spark-server/Dockerfile`:

```Dockerfile
FROM bitnami/spark:latest
USER root
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh
CMD ["/entrypoint.sh"]
```

`spark-server/entrypoint.sh`:

```bash
#!/bin/bash
/opt/bitnami/read/bin/read-submit   --class org.apache.read.sql.connect.service.SparkConnectServer   --conf read.sql.connect.enable=true   --conf read.sql.connect.grpc.binding=0.0.0.0:15002   /opt/bitnami/read/jars/read-connect_2.12-4.0.0.jar
```

Example data in `/example_data`:

- `people.tsv`
- `purchases.tsv`

## üìÇ Examples

We include a set of TypeScript scripts in the `examples/` folder:

| File            | Description                               |
|-----------------|-------------------------------------------|
| `join.ts`       | Inner join between two datasets           |
| `groupBy.ts`    | Aggregations with `groupBy` and `orderBy` |
| `withColumn.ts` | Adding and renaming columns               |
| `union.ts`      | `SetOperation` with UNION queries         |
| `distinct.ts`   | Using `distinct()` and `dropDuplicates()` |

Run any example with:

```bash
npx tsx examples/join.ts
```

## üß™ Quick Usage

```ts
import {spark} from "./src/read/session";
import {col} from "./src/engine/column";

const people = spark.read
    .option("delimiter", "\t")
    .option("header", "true")
    .csv("/data/people.tsv");

const purchases = spark.read
    .option("delimiter", "\t")
    .option("header", "true")
    .csv("/data/purchases.tsv");

people
    .join(purchases, col("id").eq(col("user_id")), "left")
    .select("name", "product", "amount")
    .filter(col("amount").gt(100))
    .show();
```

### Example: UNION with SetOperation

```ts
const p2024 = purchases.filter(col("year").eq(2024));
const p2025 = purchases.filter(col("year").eq(2025));

p2024.union(p2025, {is_all: true, by_name: false})
    .limit(5)
    .show();
```

## üí° Column Expressions

```ts
col("age").gt(18)
col("country").eq("AR")
col("age").gt(18).and(col("active").eq(true)).alias("eligible")
```

## üß† Tagless Final DSL

```ts
function userQuery<F>(dsl: DataFrameDSL<F>): F {
    return dsl
        .select(["name", "age"])
        .filter("age > 18")
        .withColumn("eligible", col("age").gt(18).and(col("country").eq("AR")));
}
```

## ‚úÖ Status Features & Roadmap

## Legend

- **P0** = Paridad base inmediata
- **P1** = I/O realista
- **P2** = Performance & DX
- **P3** = SQL/Cat√°logo & control
- **P4** = UDF (scalar / vectorizadas)
- **P5** = Streaming / Lakehouse / JDBC
- **P6** = MLlib
- **‚Äî** = ya implementado

## Feature Matrix

| Feature                                                                | Supported                                                       | Priority |
|------------------------------------------------------------------------|-----------------------------------------------------------------|----------|
| CSV Reading                                                            | ‚úÖ                                                               | ‚Äî        |
| Filtering                                                              | ‚úÖ                                                               | ‚Äî        |
| Projection / Alias                                                     | ‚úÖ                                                               | ‚Äî        |
| Arrow decoding                                                         | ‚úÖ (`.show()` prints tabular output)                             | ‚Äî        |
| Column expressions                                                     | ‚úÖ (`col`, `.gt`, `.and`, `.alias`, etc.)                        | ‚Äî        |
| DSL abstraction                                                        | ‚úÖ Tagless Final                                                 | ‚Äî        |
| Join                                                                   | ‚úÖ Supports all join types                                       | ‚Äî        |
| Aggregation                                                            | ‚úÖ (`groupBy().agg({...})`)                                      | ‚Äî        |
| Distinct                                                               | ‚úÖ (`distinct()`, `dropDuplicates(...)`)                         | ‚Äî        |
| Sorting                                                                | ‚úÖ (`orderBy(...)`, `sort(...)`)                                 | ‚Äî        |
| Limit & Take                                                           | ‚úÖ (`limit(n)`)                                                  | ‚Äî        |
| **SetOperation**                                                       | ‚úÖ (`UNION`, `INTERSECT`, `EXCEPT`)                              | ‚Äî        |
| Column renaming                                                        | ‚úÖ (`withColumnRenamed(...)`)                                    | ‚Äî        |
| Type declarations                                                      | ‚úÖ `.d.ts` published to NPM                                      | ‚Äî        |
| Modular compiler core                                                  | ‚úÖ (`engine/` separated from Spark backend)                      | ‚Äî        |
| NPM Package                                                            | ‚úÖ [Published](https://www.npmjs.com/package/ts-spark-connector) | ‚Äî        |
| Tests (Unit + Integration)                                             | ‚úÖ                                                               | ‚Äî        |
| **withColumn(...)**                                                    | ‚úÖ                                                               | ‚Äî        |
| **when(...).otherwise(...)** (CASE WHEN)                               | ‚úÖ                                                               | ‚Äî        |
| **Window functions** (`over`, `partitionBy`, `orderBy`, `rowsBetween`) | ‚úÖ                                                               | ‚Äî        |
| **Null handling** (`na.drop`, `na.fill`, `na.replace`, `isNull`)       | ‚úÖ                                                               | ‚Äî        |
| **Parquet Reading**                                                    | ‚úÖ                                                               | ‚Äî        |
| **JSON Reading**                                                       | ‚úÖ                                                               | ‚Äî        |
| **DataFrameWriter** (CSV/JSON/Parquet/ORC)                             | ‚úÖ                                                               | ‚Äî        |
| Write `partitionBy`, `bucketBy`, `sortBy`                              | ‚úÖ                                                               | ‚Äî        |
| **describe()**, `summary()`                                            | ‚ùå Not yet                                                       | **P2**   |
| **unionByName(...)**                                                   | ‚ùå Not yet                                                       | **P2**   |
| **Complex types** (arrays/maps/struct) + `explode/posexplode`          | ‚ùå Not yet                                                       | **P2**   |
| **JSON helpers** (`from_json`, `to_json`)                              | ‚ùå Not yet                                                       | **P2**   |
| **cache() / persist() / unpersist()**                                  | ‚ùå Not yet                                                       | **P2**   |
| **repartition(...) / coalesce(...)**                                   | ‚ùå Not yet                                                       | **P2**   |
| **explain(...)** (`simple/extended/formatted`)                         | ‚ùå Not yet                                                       | **P2**   |
| `SparkSession.builder.config(...)`                                     | ‚ùå Not yet                                                       | **P2**   |
| Auth/TLS for Spark Connect                                             | ‚ùå Not yet                                                       | **P2**   |
| **spark.sql(...)**                                                     | ‚ùå Not yet                                                       | **P3**   |
| Temp views (`createOrReplaceTempView`)                                 | ‚ùå Not yet                                                       | **P3**   |
| Catalog (`read.table`, `saveAsTable`)                                  | ‚ùå Not yet                                                       | **P3**   |
| Plan viz / AST dump                                                    | ‚ùå Not yet                                                       | **P3**   |
| **Join hints** (`broadcast`, `shuffle_replicate_nl`, etc.)             | ‚ùå Not yet                                                       | **P3**   |
| **sample(...)**, `randomSplit(...)`                                    | ‚ùå Not yet                                                       | **P3**   |
| UDF (scalar)                                                           | ‚ùå Not yet                                                       | **P4**   |
| **UDAF / Vectorized UDF (Arrow)**                                      | ‚ùå Not yet                                                       | **P4**   |
| Structured Streaming (`readStream` / `writeStream`)                    | ‚ùå Not yet                                                       | **P5**   |
| Watermark / trigger / output modes                                     | ‚ùå Not yet                                                       | **P5**   |
| Lakehouse: Delta/Iceberg/Hudi (`format(...)`)                          | ‚ùå Not yet                                                       | **P5**   |
| JDBC read/write (`format("jdbc")`)                                     | ‚ùå Not yet                                                       | **P5**   |
| **MLlib** (Pipelines/Transformers/Estimators b√°sicos)                  | ‚ùå Not yet                                                       | **P6**   |

---

## Roadmap por etapas (P0 ‚Üí P6)

### P0 ‚Äî Immediate base parity

**Goal:** match PySpark‚Äôs minimum ergonomics for DataFrame/Column.  
**Includes:** `withColumn`, `when/otherwise`, **window functions**, `na.*`, **tests** (unit + integration).  
**Acceptance criteria:**

- ‚úÖ `withColumn` supports adding and replacing columns.
- ‚úÖ `when(...).otherwise(...)` builds conditional expressions in `select`/`withColumn`.
- ‚úÖ Windows: `over(partitionBy(...).orderBy(...).rowsBetween(...))` with aggregations.
- ‚úÖ `na.drop/fill/replace` + `isNull/isNotNull` working.
- ‚úÖ Test suite running in CI against a local Spark Connect.
  **Notes:** Add README examples + an `df.explain()` stub for early debugging if helpful.

### P1 ‚Äî Realistic I/O

**Goal:** work with common data formats and write results.  
**Includes:** **Parquet/JSON reading**, **DataFrameWriter** (CSV/JSON/Parquet/ORC), `partitionBy/bucketBy/sortBy` (
write).  
**Criteria:**

- ‚úÖ `spark.read.parquet/json/csv(...)` with common `options(...)`.
- ‚úÖ `df.write.format(...).mode("overwrite|append").save(...)`.
- ‚úÖ `partitionBy` on write and smoke tests that read back what was written.
  **Notes:** Cover basic schema inference and clear error messages.

### P2 ‚Äî Performance & DX

**Goal:** partition control, caching, and better debugging.  
**Includes:** `cache/persist/unpersist`, `repartition/coalesce`, **explain(formatted)**, `describe/summary`,
`unionByName`, **complex types + explode**, **JSON helpers**, `SparkSession.builder.config`, **Auth/TLS**.  
**Criteria:**

- ‚úÖ Partition changes reflected in the plan.
- ‚úÖ `cache/persist` maintain consistency; `unpersist` cleans up.
- ‚úÖ `explain("formatted")`/`df.explain()` shows a valid plan.
- ‚úÖ `from_json/to_json` + `explode` for arrays/maps/structs.
  **Notes:** Document tuning recommendations and DX examples.

### P3 ‚Äî SQL/Catalog & fine-grained control

**Goal:** full interop with SQL and catalog.  
**Includes:** **spark.sql(...)**, **temp views**, **catalog** (`read.table`, `saveAsTable`), **plan viz/AST dump**, *
*join hints**, `sample/randomSplit`.  
**Criteria:**

- ‚úÖ `spark.sql("...")` yields a `DataFrame` equivalent to the DSL.
- ‚úÖ `createOrReplaceTempView` usable from `spark.sql`.
- ‚úÖ `broadcast(df2)`/hints reflected in the plan.
  **Notes:** Optional: command to dump the logical plan in JSON/Protobuf.

### P4 ‚Äî UDF (User-Defined Functions)

**Goal:** extend transformations with custom logic.  
**Includes:** **Scalar UDF**, **UDAF / vectorized UDF (Arrow)**.  
**Criteria:**

- ‚úÖ Register and use UDFs in the DSL and in `spark.sql`.
- ‚úÖ Vectorized UDF with Arrow works on the happy path and passes interoperability tests.
  **Notes:** Document costs/limitations and best practices.

### P5 ‚Äî Streaming / Lakehouse / JDBC

**Goal:** enable production pipelines.  
**Includes:** **Structured Streaming** (`readStream`/`writeStream`, **watermark**, **trigger**, **output modes**), *
*Delta/Iceberg/Hudi** via `format(...)`, **JDBC** read/write.  
**Criteria:**

- ‚úÖ End-to-end streaming example (socket/Kafka ‚Üí transform ‚Üí sink).
- ‚úÖ Read/write to Delta/Iceberg/Hudi when the cluster has the required JARs.
- ‚úÖ JDBC tested against a popular database (Postgres/MySQL).

### P6 ‚Äî MLlib

**Goal:** ML pipelines on DataFrames from TypeScript.  
**Includes:** Pipelines, basic Estimators/Transformers (e.g., `StringIndexer`, `VectorAssembler`,
`LogisticRegression`).  
**Criteria:**

- ‚úÖ Train, save, and load models; `transform()` on a DataFrame.
- ‚úÖ Reproducible example (public dataset) and migration guide from PySpark.

---

## üìÑ License

Apache-2.0 ‚Äî This project aims to democratize access to Spark features from the TypeScript ecosystem.
