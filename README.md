# ts-spark-connector

üå± **Status: Alpha ‚Äì Early growth stage**

TypeScript client for [Apache Spark Connect](https://spark.apache.org/docs/latest/sql-connect.html).  
Construct Spark logical plans entirely in TypeScript and run them against a Spark Connect server.

## üöÄ Features

- Build Spark logical plans using a fluent, PySpark-style API in TypeScript
- Evaluate transformations locally or stream results via Arrow
- Tagless Final DSL design with support for multiple backends
- Composable, immutable, and strongly typed DataFrame operations
- Supports column expressions (`col`, `.gt`, `.alias`, `.and`, etc.)
- Compatible with Spark Connect Protobuf and
  `spark-submit --class org.apache.spark.sql.connect.service.SparkConnectServer`
- **Supports Spark SetOperations** (`UNION`, `INTERSECT`, `EXCEPT`) with `by_name`, `is_all`, and
  `allow_missing_columns`
- Support for Spark-compatible joins with configurable join types
- Session-aware execution without relying on global singletons
- Includes **ready-to-run examples** in the `examples/` folder

## üì¶ Installation

```bash
git clone https://github.com/your-org/ts-spark-connector
cd ts-read-connector
npm install
```

## üîß Docker Setup

You need a Spark Connect server running. Example `docker-compose.yml`:

```yaml
services:
  spark:
    build: ./read-server
    ports:
      - "15002:15002"
    volumes:
      - ./example_data:/data
    environment:
      - SPARK_NO_DAEMONIZE=true
```

`spark-server/Dockerfile`:

```Dockerfile
FROM bitnami/spark:latest
USER root
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh
CMD ["/entrypoint.sh"]
```

`spark-server/entrypoint.sh`:

```bash
#!/bin/bash
/opt/bitnami/read/bin/read-submit   --class org.apache.read.sql.connect.service.SparkConnectServer   --conf read.sql.connect.enable=true   --conf read.sql.connect.grpc.binding=0.0.0.0:15002   /opt/bitnami/read/jars/read-connect_2.12-4.0.0.jar
```

Example data in `/example_data`:

- `people.tsv`
- `purchases.tsv`

## üìÇ Examples

We include a set of TypeScript scripts in the `examples/` folder:

| File            | Description                               |
|-----------------|-------------------------------------------|
| `join.ts`       | Inner join between two datasets           |
| `groupBy.ts`    | Aggregations with `groupBy` and `orderBy` |
| `withColumn.ts` | Adding and renaming columns               |
| `union.ts`      | `SetOperation` with UNION queries         |
| `distinct.ts`   | Using `distinct()` and `dropDuplicates()` |

Run any example with:

```bash
npx tsx examples/join.ts
```

## üß™ Quick Usage

```ts
import {spark} from "./src/read/session";
import {col} from "./src/engine/column";

const people = spark.read
    .option("delimiter", "\t")
    .option("header", "true")
    .csv("/data/people.tsv");

const purchases = spark.read
    .option("delimiter", "\t")
    .option("header", "true")
    .csv("/data/purchases.tsv");

people
    .join(purchases, col("id").eq(col("user_id")), "left")
    .select("name", "product", "amount")
    .filter(col("amount").gt(100))
    .show();
```

### Example: UNION with SetOperation

```ts
const p2024 = purchases.filter(col("year").eq(2024));
const p2025 = purchases.filter(col("year").eq(2025));

p2024.union(p2025, {is_all: true, by_name: false})
    .limit(5)
    .show();
```

## üí° Column Expressions

```ts
col("age").gt(18)
col("country").eq("AR")
col("age").gt(18).and(col("active").eq(true)).alias("eligible")
```

## üß† Tagless Final DSL

```ts
function userQuery<F>(dsl: DataFrameDSL<F>): F {
    return dsl
        .select(["name", "age"])
        .filter("age > 18")
        .withColumn("eligible", col("age").gt(18).and(col("country").eq("AR")));
}
```

## ‚úÖ Status Features & Roadmap

## Legend

- **P0** = Paridad base inmediata
- **P1** = I/O realista
- **P2** = Performance & DX
- **P3** = SQL/Cat√°logo & control
- **P4** = UDF (scalar / vectorizadas)
- **P5** = Streaming / Lakehouse / JDBC
- **P6** = MLlib
- **‚Äî** = ya implementado

## Feature Matrix

| Feature                                                                | Supported                                                       | Priority |
|------------------------------------------------------------------------|-----------------------------------------------------------------|----------|
| CSV Reading                                                            | ‚úÖ                                                               | ‚Äî        |
| Filtering                                                              | ‚úÖ                                                               | ‚Äî        |
| Projection / Alias                                                     | ‚úÖ                                                               | ‚Äî        |
| Arrow decoding                                                         | ‚úÖ (`.show()` prints tabular output)                             | ‚Äî        |
| Column expressions                                                     | ‚úÖ (`col`, `.gt`, `.and`, `.alias`, etc.)                        | ‚Äî        |
| DSL abstraction                                                        | ‚úÖ Tagless Final                                                 | ‚Äî        |
| Join                                                                   | ‚úÖ Supports all join types                                       | ‚Äî        |
| Aggregation                                                            | ‚úÖ (`groupBy().agg({...})`)                                      | ‚Äî        |
| Distinct                                                               | ‚úÖ (`distinct()`, `dropDuplicates(...)`)                         | ‚Äî        |
| Sorting                                                                | ‚úÖ (`orderBy(...)`, `sort(...)`)                                 | ‚Äî        |
| Limit & Take                                                           | ‚úÖ (`limit(n)`)                                                  | ‚Äî        |
| **SetOperation**                                                       | ‚úÖ (`UNION`, `INTERSECT`, `EXCEPT`)                              | ‚Äî        |
| Column renaming                                                        | ‚úÖ (`withColumnRenamed(...)`)                                    | ‚Äî        |
| Type declarations                                                      | ‚úÖ `.d.ts` published to NPM                                      | ‚Äî        |
| Modular compiler core                                                  | ‚úÖ (`engine/` separated from Spark backend)                      | ‚Äî        |
| NPM Package                                                            | ‚úÖ [Published](https://www.npmjs.com/package/ts-spark-connector) | ‚Äî        |
| Tests (Unit + Integration)                                             | ‚úÖ                                                               | ‚Äî        |
| **withColumn(...)**                                                    | ‚úÖ                                                               | ‚Äî        |
| **when(...).otherwise(...)** (CASE WHEN)                               | ‚úÖ                                                               | ‚Äî        |
| **Window functions** (`over`, `partitionBy`, `orderBy`, `rowsBetween`) | ‚úÖ                                                               | ‚Äî        |
| **Null handling** (`na.drop`, `na.fill`, `na.replace`, `isNull`)       | ‚úÖ                                                               | ‚Äî        |
| **Parquet Reading**                                                    | ‚úÖ                                                               | ‚Äî        |
| **JSON Reading**                                                       | ‚úÖ                                                               | ‚Äî        |
| **DataFrameWriter** (CSV/JSON/Parquet/ORC)                             | ‚úÖ                                                               | ‚Äî        |
| Write `partitionBy`, `bucketBy`, `sortBy`                              | ‚úÖ                                                               | ‚Äî        |
| **describe()**, `summary()`                                            | ‚úÖ                                                               | ‚Äî        |
| **unionByName(...)**                                                   | ‚úÖ                                                               | ‚Äî        |
| **Complex types** (arrays/maps/struct) + `explode/posexplode`          | ‚úÖ                                                               | ‚Äî        |
| **JSON helpers** (`from_json`, `to_json`)                              | ‚úÖ                                                               | **P2**   |
| **cache() / persist() / unpersist()**                                  | ‚ùå Not yet                                                       | **P2**   |
| **repartition(...) / coalesce(...)**                                   | ‚ùå Not yet                                                       | **P2**   |
| **explain(...)** (`simple/extended/formatted`)                         | ‚ùå Not yet                                                       | **P2**   |
| `SparkSession.builder.config(...)`                                     | ‚ùå Not yet                                                       | **P2**   |
| Auth/TLS for Spark Connect                                             | ‚ùå Not yet                                                       | **P2**   |
| **spark.sql(...)**                                                     | ‚ùå Not yet                                                       | **P3**   |
| Temp views (`createOrReplaceTempView`)                                 | ‚ùå Not yet                                                       | **P3**   |
| Catalog (`read.table`, `saveAsTable`)                                  | ‚úÖ                                                               | ‚Äî        |
| Plan viz / AST dump                                                    | ‚ùå Not yet                                                       | **P3**   |
| **Join hints** (`broadcast`, `shuffle_replicate_nl`, etc.)             | ‚ùå Not yet                                                       | **P3**   |
| **sample(...)**, `randomSplit(...)`                                    | ‚ùå Not yet                                                       | **P3**   |
| UDF (scalar)                                                           | ‚ùå Not yet                                                       | **P4**   |
| **UDAF / Vectorized UDF (Arrow)**                                      | ‚ùå Not yet                                                       | **P4**   |
| Structured Streaming (`readStream` / `writeStream`)                    | ‚ùå Not yet                                                       | **P5**   |
| Watermark / trigger / output modes                                     | ‚ùå Not yet                                                       | **P5**   |
| Lakehouse: Delta/Iceberg/Hudi (`format(...)`)                          | ‚ùå Not yet                                                       | **P5**   |
| JDBC read/write (`format("jdbc")`)                                     | ‚ùå Not yet                                                       | **P5**   |
| **MLlib** (Pipelines/Transformers/Estimators b√°sicos)                  | ‚ùå Not yet                                                       | **P6**   |

## üìÑ License

Apache-2.0 ‚Äî This project aims to democratize access to Spark features from the TypeScript ecosystem.
