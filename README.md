# ts-spark-connector

TypeScript client for [Apache Spark Connect](https://spark.apache.org/docs/latest/sql-connect.html).  
Construct Spark logical plans entirely in TypeScript and run them against a Spark Connect server.

## üöÄ Features

- Build Spark logical plans using a fluent, PySpark-style API in TypeScript
- Evaluate transformations locally or stream results via Arrow
- Tagless Final DSL design with support for multiple backends
- Composable, immutable, and strongly typed DataFrame operations
- Column expressions (`col`, `.gt`, `.alias`, `.and`, etc.)
- Compatible with Spark Connect Protobuf and
  `spark-submit --class org.apache.spark.sql.connect.service.SparkConnectServer`
- **Set operations** (`UNION`, `INTERSECT`, `EXCEPT`) with `by_name`, `is_all`, and `allow_missing_columns`
- Spark-compatible joins with configurable join types
- Session-aware execution (no global singletons)
- **Plan viz / AST dump**: export client AST to JSON & Mermaid
- **Ready-to-run examples** in `examples/`

## üì¶ Installation

```bash
git clone https://github.com/BaldrVivaldelli/ts-spark-connector
cd ts-spark-connector
npm install
```

> You need a running **Spark Connect** server. See [`spark-server/README.md`](spark-server/README.md) for a ready-to-use Docker setup, or run your own server.

## üß™ Quick Start

```ts
import { SparkSession } from "./src/client/session";
import { col } from "./src/engine/column";

const session = SparkSession.builder()
  // optional: auth / TLS
  .getOrCreate();

const people = session.read
  .option("delimiter", "\t")
  .option("header", "true")
  .csv("/data/people.tsv");

const purchases = session.read
  .option("delimiter", "\t")
  .option("header", "true")
  .csv("/data/purchases.tsv");

await people
  .join(purchases, col("id").eq(col("user_id")), "left")
  .select("name", "product", "amount")
  .filter(col("amount").gt(100))
  .show();
```

### Example: UNION (Set Operation)

```ts
const p2024 = purchases.filter(col("year").eq(2024));
const p2025 = purchases.filter(col("year").eq(2025));

await p2024.union(p2025, { is_all: true, by_name: false })
  .limit(5)
  .show();
```

## üó∫Ô∏è Plan viz / AST dump

Inspect the **client-side** plan (before server optimization):

```ts
const df = purchases
  .select("user_id", "product", "amount")
  .filter(col("amount").gt(100))
  .orderBy(col("user_id").descNullsLast());

console.log(df.toClientASTJSON());      // JSON AST
console.log(df.toClientASTMermaid());   // Mermaid diagram
console.log(df.toSparkLogicalPlanJSON());// Client logical plan
console.log(df.toProtoJSON());           // Spark Connect proto
```

> Tip: write these strings to disk (`.mmd`, `.json`) and publish them as CI artifacts.

## üîê TLS

```ts
const session = SparkSession.builder()
  .enableTLS({
    keyStorePath: "./certs/keystore.p12",
    keyStorePassword: "password",
    trustStorePath: "./certs/cert.crt",
    trustStorePassword: "password",
  })
  .getOrCreate();
```

## ‚úÖ Compatibility Matrix

| Component        | Supported / Tested                 |
|------------------|------------------------------------|
| Spark Connect    | 3.5.x                              |
| Scala ABI (JAR)  | 2.12 (`spark-connect_2.12`)        |
| Node.js          | 18, 20, 22                         |
| OS               | Linux (CI); macOS (local)          |

> Planned: add CI jobs for macOS/Windows; update table as coverage expands.

## ‚úÖ Feature Matrix

| Feature                                                                | Supported |
|------------------------------------------------------------------------|-----------|
| CSV Reading                                                            | ‚úÖ         |
| Filtering                                                              | ‚úÖ         |
| Projection / Alias                                                     | ‚úÖ         |
| Arrow decoding (`.show()`)                                             | ‚úÖ         |
| Column expressions (`col`, `.gt`, `.and`, `.alias`, etc.)              | ‚úÖ         |
| DSL abstraction (Tagless Final)                                        | ‚úÖ         |
| Joins (configurable types)                                             | ‚úÖ         |
| Aggregation (`groupBy().agg({...})`)                                   | ‚úÖ         |
| Distinct (`distinct()`, `dropDuplicates(...)`)                         | ‚úÖ         |
| Sorting (`orderBy(...)`, `sort(...)`)                                  | ‚úÖ         |
| Limit (`limit(n)`)                                                     | ‚úÖ         |
| **Set operations** (`UNION`, `INTERSECT`, `EXCEPT`)                    | ‚úÖ         |
| Column renaming (`withColumnRenamed(...)`)                             | ‚úÖ         |
| Type declarations (`.d.ts`)                                            | ‚úÖ         |
| Modular compiler core (backend-agnostic)                               | ‚úÖ         |
| Tests (Unit + Integration + E2E)                                       | ‚úÖ         |
| **withColumn(...)**                                                    | ‚úÖ         |
| **when(...).otherwise(...)**                                           | ‚úÖ         |
| **Window functions**                                                   | ‚úÖ         |
| **Null handling** (`isNull`, `na.drop/fill/replace`)                   | ‚úÖ         |
| **Parquet Reading**                                                    | ‚úÖ         |
| **JSON Reading**                                                       | ‚úÖ         |
| **DataFrameWriter** (CSV/JSON/Parquet/ORC/Avro)                        | ‚úÖ         |
| Write `partitionBy`, `bucketBy`, `sortBy`                              | ‚úÖ         |
| **describe()**, `summary()`                                            | ‚úÖ         |
| **unionByName(...)**                                                   | ‚úÖ         |
| **Complex types** + `explode/posexplode`                               | ‚úÖ         |
| **JSON helpers** (`from_json`, `to_json`)                              | ‚úÖ         |
| **repartition(...) / coalesce(...)**                                   | ‚úÖ         |
| **explain(...)** (`simple/extended/formatted`)                         | ‚úÖ         |
| `SparkSession.builder.config(...)`                                     | ‚úÖ         |
| Auth/TLS for Spark Connect                                             | ‚úÖ         |
| **spark.sql(...)**                                                     | ‚úÖ         |
| Temp views (`createOrReplaceTempView`)                                 | ‚úÖ         |
| Catalog (`read.table`, `saveAsTable`)                                  | ‚úÖ         |
| **Plan viz / AST dump**                                                | ‚úÖ         |
| **cache() / persist() / unpersist()**                                  | ‚ö†Ô∏è Limited by Spark Connect |
| **Join hints** (`broadcast`, etc.)                                     | ‚úÖ         |
| **sample(...)**, `randomSplit(...)`                                    | ‚úÖ         |
| UDF (scalar)                                                           | ‚ö†Ô∏è Limited by Spark Connect |
| **UDAF / Vectorized UDF (Arrow)**                                      | ‚ö†Ô∏è Limited by Spark Connect |
| Structured Streaming                                                   | ‚úÖ         |
| Watermark / triggers / output modes                                    | ‚úÖ         |
| Lakehouse: Delta/Iceberg/Hudi                                          | ‚ùå         |
| JDBC read/write                                                        | ‚ùå         |
| **MLlib**                                                              | ‚ùå         |

## üìÑ License

Apache-2.0
